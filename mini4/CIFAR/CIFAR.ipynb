{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import torchvision\nimport warnings\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport numpy as np\n\nfrom tqdm import tqdm\n\nfrom torch import nn, optim\nimport torch\n\n\n\n###########################\n# Dataset class\n###########################\nclass IMBALANCECIFAR10(torchvision.datasets.CIFAR10):\n    cls_num = 10\n\n    def __init__(self, root, imb_type='exp', imb_factor=0.01, rand_number=0, train=True,\n                 transform=None, target_transform=None,\n                 download=False):\n        super(IMBALANCECIFAR10, self).__init__(root, train, transform, target_transform, download)\n        np.random.seed(rand_number)\n        img_num_list = self.get_img_num_per_cls(self.cls_num, imb_type, imb_factor)\n        self.gen_imbalanced_data(img_num_list)\n\n    def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):\n        img_max = len(self.data) / cls_num\n        img_num_per_cls = []\n        if imb_type == 'exp':\n            for cls_idx in range(cls_num):\n                num = img_max * (imb_factor ** (cls_idx / (cls_num - 1.0)))\n                img_num_per_cls.append(int(num))\n        elif imb_type == 'step':\n            for cls_idx in range(cls_num // 2):\n                img_num_per_cls.append(int(img_max))\n            for cls_idx in range(cls_num // 2):\n                img_num_per_cls.append(int(img_max * imb_factor))\n        else:\n            img_num_per_cls.extend([int(img_max)] * cls_num)\n        return img_num_per_cls\n\n    def gen_imbalanced_data(self, img_num_per_cls):\n        new_data = []\n        new_targets = []\n        targets_np = np.array(self.targets, dtype=np.int64)\n        classes = np.unique(targets_np)\n        # np.random.shuffle(classes)\n        self.num_per_cls_dict = dict()\n        for the_class, the_img_num in zip(classes, img_num_per_cls):\n            self.num_per_cls_dict[the_class] = the_img_num\n            idx = np.where(targets_np == the_class)[0]\n            np.random.shuffle(idx)\n            selec_idx = idx[:the_img_num]\n            new_data.append(self.data[selec_idx, ...])\n            new_targets.extend([the_class, ] * the_img_num)\n        new_data = np.vstack(new_data)\n        self.data = new_data\n        self.targets = new_targets\n\n    def get_cls_num_list(self):\n        cls_num_list = []\n        for i in range(self.cls_num):\n            cls_num_list.append(self.num_per_cls_dict[i])\n        return cls_num_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# losses\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\ndef focal_loss(input_values, gamma):\n    \"\"\"Computes the focal loss\"\"\"\n    p = torch.exp(-input_values)\n    loss = (1 - p) ** gamma * input_values\n    return loss.mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, weight=None, gamma=0.):\n        super(FocalLoss, self).__init__()\n        assert gamma >= 0\n        self.gamma = gamma\n        self.weight = weight\n\n    def forward(self, input, target):\n        return focal_loss(F.cross_entropy(input, target, reduction='none', weight=self.weight), self.gamma)\n\nclass LDAMLoss(nn.Module):\n    \n    def __init__(self, cls_num_list, max_m=0.5, weight=None, s=30):\n        super(LDAMLoss, self).__init__()\n        m_list = 1.0 / np.sqrt(np.sqrt(cls_num_list))\n        m_list = m_list * (max_m / np.max(m_list))\n        m_list = torch.cuda.FloatTensor(m_list)\n        self.m_list = m_list\n        assert s > 0\n        self.s = s\n        self.weight = weight\n\n    def forward(self, x, target):\n        index = torch.zeros_like(x, dtype=torch.uint8)\n        index.scatter_(1, target.data.view(-1, 1), 1)\n        \n        index_float = index.type(torch.cuda.FloatTensor)\n        batch_m = torch.matmul(self.m_list[None, :], index_float.transpose(0,1))\n        batch_m = batch_m.view((-1, 1))\n        x_m = x - batch_m\n    \n        output = torch.where(index, x_m, x)\n        return F.cross_entropy(self.s*output, target, weight=self.weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.nn import Parameter\n\n__all__ = ['ResNet_s', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n\ndef _weights_init(m):\n    classname = m.__class__.__name__\n    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n        init.kaiming_normal_(m.weight)\n\nclass NormedLinear(nn.Module):\n\n    def __init__(self, in_features, out_features):\n        super(NormedLinear, self).__init__()\n        self.weight = Parameter(torch.Tensor(in_features, out_features))\n        self.weight.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)\n\n    def forward(self, x):\n        out = F.normalize(x, dim=1).mm(F.normalize(self.weight, dim=0))\n        return out\n\nclass LambdaLayer(nn.Module):\n\n    def __init__(self, lambd):\n        super(LambdaLayer, self).__init__()\n        self.lambd = lambd\n\n    def forward(self, x):\n        return self.lambd(x)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, option='A'):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            if option == 'A':\n                \"\"\"\n                For CIFAR10 ResNet paper uses option A.\n                \"\"\"\n                self.shortcut = LambdaLayer(lambda x:\n                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n            elif option == 'B':\n                self.shortcut = nn.Sequential(\n                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                     nn.BatchNorm2d(self.expansion * planes)\n                )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet_s(nn.Module):\n\n    def __init__(self, block, num_blocks, num_classes=10, use_norm=False):\n        super(ResNet_s, self).__init__()\n        self.in_planes = 16\n\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n        if use_norm:\n            self.linear = NormedLinear(64, num_classes)\n        else:\n            self.linear = nn.Linear(64, num_classes)\n        self.apply(_weights_init)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.avg_pool2d(out, out.size()[3])\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\n\n\ndef resnet32(num_classes=10, use_norm=False):\n    return ResNet_s(BasicBlock, [5, 5, 5], num_classes=num_classes, use_norm=use_norm)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###########\n# helper functions\n############\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nimport inspect\n\ndef adjust_learning_rate(optimizer, epoch, lr):\n    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n    epoch = epoch + 1\n    if epoch <= 5:\n        lr = lr * epoch / 5\n    elif epoch > 180:\n        lr = lr * 0.0001\n    elif epoch > 160:\n        lr = lr * 0.01\n    else:\n        lr = lr\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n        \n        \ndef accuracy(output, target, topk=(1,)):\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\ndef calculate_metric(metric_fn, true_y, pred_y):\n    if \"average\" in inspect.getfullargspec(metric_fn).args:\n        return metric_fn(true_y, pred_y, average=\"macro\")\n    else:\n        return metric_fn(true_y, pred_y)\n\n\ndef print_scores(p, r, f1, a, batch_size):\n    for name, scores in zip((\"precision\", \"recall\", \"F1\", \"accuracy\"), (p, r, f1, a)):\n        print(f\"\\t{name.rjust(14, ' ')}: {sum(scores) / batch_size:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############\n# main\n##############\nfrom sklearn.metrics import confusion_matrix\nif __name__ == '__main__':\n\n    # data setup\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_val = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n\n    train_dataset = IMBALANCECIFAR10(root='../input/cifar10-python', imb_type='exp', imb_factor=0.01,\n                                     rand_number=0, train=True,\n                                     transform=transform_train)\n\n    val_dataset = datasets.CIFAR10(root='../input/cifar10-python', train=False, transform=transform_val)\n\n    print(len(train_dataset))  # 20431\n    print(len(val_dataset))  # 10000\n    cls_num_list = train_dataset.get_cls_num_list()\n    print('cls num list:')\n    print(cls_num_list)\n\n\n\n\n    # define train rule\n    train_rule = 'None'\n    train_sampler = None\n    if train_rule == 'None':\n        train_sampler = None\n        per_cls_weights = None\n    elif train_rule == 'Resample':\n        train_sampler = ImbalancedDatasetSampler(train_dataset)\n        per_cls_weights = None\n    elif train_rule == 'Reweight':\n        train_sampler = None\n        beta = 0.9999\n        effective_num = 1.0 - np.power(beta, cls_num_list)\n        per_cls_weights = (1.0 - beta) / np.array(effective_num)\n        per_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(cls_num_list)\n        per_cls_weights = torch.FloatTensor(per_cls_weights)\n    else:\n        warnings.warn('Sample rule is not listed')\n\n\n\n\n    # define loss function: focal loss, cross entropy\n    loss_type = 'CE'\n    criterion = None\n    if loss_type == 'CE':\n        criterion = nn.CrossEntropyLoss(weight=per_cls_weights)\n    elif loss_type == 'Focal':\n        criterion = FocalLoss(weight=per_cls_weights, gamma=1)\n    else:\n        warnings.warn('Loss type is not listed')\n\n\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=30, shuffle=(train_sampler is None),\n        num_workers=2, pin_memory=True, sampler=train_sampler)\n\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=30, shuffle=False,\n        num_workers=2, pin_memory=True)\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = resnet32(num_classes=len(cls_num_list), use_norm=True).to(device)\n    optimizer = torch.optim.SGD(model.parameters(), 0.1,\n                                momentum=0.9,\n                                weight_decay=2e-4)\n\n    losses = []\n    epochs = 200\n\n\n    batches = len(train_loader)\n    val_batches = len(val_loader)\n\n\n    for epoch in range(0, epochs + 1):\n        adjust_learning_rate(optimizer, epoch, 0.1)\n\n\n        # switch to train mode\n        total_loss = 0\n        #progress = tqdm(enumerate(train_loader), desc=\"Loss: \", total=batches)\n        model.train()\n        \n        \n        for i, (input, target) in enumerate(train_loader):\n            input = input.to(device)\n            target = target.to(device)\n            model.zero_grad()\n\n            # compute output\n            output = model(input)\n            loss = criterion(output, target)\n            \n           \n            \n            # compute gradient and do SGD step\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            current_loss = loss.item()\n            total_loss += current_loss\n            #progress.update(1)\n            #progress.set_description(\"Loss: {:.4f}\".format(total_loss / (i + 1)))\n            \n            \n            \n        torch.cuda.empty_cache()\n\n\n        #validation mode\n        val_losses = 0\n        precision, recall, f1, accuracy = [], [], [], []\n        \n        val_top1 = AverageMeter('Acc@1', ':6.2f')\n        val_top5 = AverageMeter('Acc@5', ':6.2f')\n        \n        \n        model.eval()\n        all_preds = []\n        all_targets = []\n\n        with torch.no_grad():\n            for i, (input, target) in enumerate(val_loader):\n                input = input.to(device)\n                target = target.to(device)\n\n                # compute output\n                output = model(input)\n                loss = criterion(output, target)\n                val_losses += loss\n\n                # measure accuracy and record loss\n                #acc = accuracy(output, target)\n\n                pred = torch.max(output, 1)[1]\n                for acc, metric in zip((precision, recall, f1, accuracy),\n                                   (precision_score, recall_score, f1_score, accuracy_score)):\n                    acc.append(calculate_metric(metric, target.cpu(), pred.cpu()))\n                \n                all_preds.extend(pred.cpu().numpy())\n                all_targets.extend(target.cpu().numpy())\n            \n        print(\n        f\"Epoch {epoch + 1}/{epochs}, training loss: {total_loss / batches}, validation loss: {val_losses / val_batches}\")\n        print_scores(precision, recall, f1, accuracy, val_batches)\n        losses.append(total_loss / batches)\n    print(losses)\n\n    '''\n            cf = confusion_matrix(all_targets, all_preds).astype(float)\n            cls_cnt = cf.sum(axis=1) #total\n            cls_hit = np.diag(cf) #hit\n            cls_acc = cls_hit / cls_cnt\n\n            out_cls_acc = '%s Class Accuracy: %s' % (\n                'val', (np.array2string(cls_acc, separator=',', formatter={'float_kind': lambda x: \"%.3f\" % x})))\n            print(out_cls_acc)\n\n        print(f\"Epoch {epoch + 1}/{epochs}, training loss: {total_loss / batches}, validation loss: {val_losses / val_batches}\")\n\n        #print(\"accuracy: %f\" %(sum(precision)/val_batches))\n\n        losses.append(total_loss / batches)\n        \n    print(losses)\n    '''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}