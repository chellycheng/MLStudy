{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import torchvision\nimport warnings\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport numpy as np\n\nfrom tqdm import tqdm\n\nfrom torch import nn, optim\nimport torch\n\n\n\n###########################\n# Dataset class\n###########################\nclass IMBALANCECIFAR10(torchvision.datasets.CIFAR10):\n    cls_num = 10\n\n    def __init__(self, root, imb_type='exp', imb_factor=0.01, rand_number=0, train=True,\n                 transform=None, target_transform=None,\n                 download=False):\n        super(IMBALANCECIFAR10, self).__init__(root, train, transform, target_transform, download)\n        np.random.seed(rand_number)\n        img_num_list = self.get_img_num_per_cls(self.cls_num, imb_type, imb_factor)\n        self.gen_imbalanced_data(img_num_list)\n\n    def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):\n        img_max = len(self.data) / cls_num\n        img_num_per_cls = []\n        if imb_type == 'exp':\n            for cls_idx in range(cls_num):\n                num = img_max * (imb_factor ** (cls_idx / (cls_num - 1.0)))\n                img_num_per_cls.append(int(num))\n        elif imb_type == 'step':\n            for cls_idx in range(cls_num // 2):\n                img_num_per_cls.append(int(img_max))\n            for cls_idx in range(cls_num // 2):\n                img_num_per_cls.append(int(img_max * imb_factor))\n        else:\n            img_num_per_cls.extend([int(img_max)] * cls_num)\n        return img_num_per_cls\n\n    def gen_imbalanced_data(self, img_num_per_cls):\n        new_data = []\n        new_targets = []\n        targets_np = np.array(self.targets, dtype=np.int64)\n        classes = np.unique(targets_np)\n        # np.random.shuffle(classes)\n        self.num_per_cls_dict = dict()\n        for the_class, the_img_num in zip(classes, img_num_per_cls):\n            self.num_per_cls_dict[the_class] = the_img_num\n            idx = np.where(targets_np == the_class)[0]\n            np.random.shuffle(idx)\n            selec_idx = idx[:the_img_num]\n            new_data.append(self.data[selec_idx, ...])\n            new_targets.extend([the_class, ] * the_img_num)\n        new_data = np.vstack(new_data)\n        self.data = new_data\n        self.targets = new_targets\n\n    def get_cls_num_list(self):\n        cls_num_list = []\n        for i in range(self.cls_num):\n            cls_num_list.append(self.num_per_cls_dict[i])\n        return cls_num_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# losses\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\ndef focal_loss(input_values, gamma):\n    \"\"\"Computes the focal loss\"\"\"\n    p = torch.exp(-input_values)\n    loss = (1 - p) ** gamma * input_values\n    return loss.mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, weight=None, gamma=0.):\n        super(FocalLoss, self).__init__()\n        assert gamma >= 0\n        self.gamma = gamma\n        self.weight = weight\n\n    def forward(self, input, target):\n        return focal_loss(F.cross_entropy(input, target, reduction='none', weight=self.weight), self.gamma)\n\nclass LDAMLoss(nn.Module):\n    \n    def __init__(self, cls_num_list, max_m=0.5, weight=None, s=30):\n        super(LDAMLoss, self).__init__()\n        m_list = 1.0 / np.sqrt(np.sqrt(cls_num_list))\n        m_list = m_list * (max_m / np.max(m_list))\n        m_list = torch.cuda.FloatTensor(m_list)\n        self.m_list = m_list\n        assert s > 0\n        self.s = s\n        self.weight = weight\n\n    def forward(self, x, target):\n        index = torch.zeros_like(x, dtype=torch.uint8)\n        index.scatter_(1, target.data.view(-1, 1), 1)\n        \n        index_float = index.type(torch.cuda.FloatTensor)\n        batch_m = torch.matmul(self.m_list[None, :], index_float.transpose(0,1))\n        batch_m = batch_m.view((-1, 1))\n        x_m = x - batch_m\n    \n        output = torch.where(index, x_m, x)\n        return F.cross_entropy(self.s*output, target, weight=self.weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.nn import Parameter\n\n__all__ = ['ResNet_s', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n\ndef _weights_init(m):\n    classname = m.__class__.__name__\n    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n        init.kaiming_normal_(m.weight)\n\nclass NormedLinear(nn.Module):\n\n    def __init__(self, in_features, out_features):\n        super(NormedLinear, self).__init__()\n        self.weight = Parameter(torch.Tensor(in_features, out_features))\n        self.weight.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)\n\n    def forward(self, x):\n        out = F.normalize(x, dim=1).mm(F.normalize(self.weight, dim=0))\n        return out\n\nclass LambdaLayer(nn.Module):\n\n    def __init__(self, lambd):\n        super(LambdaLayer, self).__init__()\n        self.lambd = lambd\n\n    def forward(self, x):\n        return self.lambd(x)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, option='A'):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            if option == 'A':\n                \"\"\"\n                For CIFAR10 ResNet paper uses option A.\n                \"\"\"\n                self.shortcut = LambdaLayer(lambda x:\n                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n            elif option == 'B':\n                self.shortcut = nn.Sequential(\n                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                     nn.BatchNorm2d(self.expansion * planes)\n                )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet_s(nn.Module):\n\n    def __init__(self, block, num_blocks, num_classes=10, use_norm=False):\n        super(ResNet_s, self).__init__()\n        self.in_planes = 16\n\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n        if use_norm:\n            self.linear = NormedLinear(64, num_classes)\n        else:\n            self.linear = nn.Linear(64, num_classes)\n        self.apply(_weights_init)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.avg_pool2d(out, out.size()[3])\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\n\n\ndef resnet32(num_classes=10, use_norm=False):\n    return ResNet_s(BasicBlock, [5, 5, 5], num_classes=num_classes, use_norm=use_norm)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###########\n# helper functions\n############\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nimport inspect\n\ndef adjust_learning_rate(optimizer, epoch, lr):\n    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n    \n    epoch = epoch + 1\n    \n    if epoch <= 5:\n        lr = lr * epoch / 5\n    elif epoch > 40:\n        lr = lr * 0.01\n    elif epoch > 25:\n        lr = lr * 0.1\n    else:\n        lr = lr\n    \n    '''\n    epoch = epoch + 1\n    if epoch <= 5:\n        lr = lr * epoch / 5\n    elif epoch > 180:\n        lr = lr * 0.0001\n    elif epoch > 160:\n        lr = lr * 0.01\n    else:\n        lr = lr\n    '''\n    \n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n       \n        \ndef accuracy(output, target, topk=(1,)):\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\ndef calculate_metric(metric_fn, true_y, pred_y):\n    if \"average\" in inspect.getfullargspec(metric_fn).args:\n        return metric_fn(true_y, pred_y, average=\"macro\")\n    else:\n        return metric_fn(true_y, pred_y)\n\n\ndef print_scores(p, r, f1, a, batch_size):\n    for name, scores in zip((\"precision\", \"recall\", \"F1\", \"accuracy\"), (p, r, f1, a)):\n        print(f\"\\t{name.rjust(14, ' ')}: {sum(scores) / batch_size:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import argparse\nimport os\nimport random\nimport time\nimport warnings\nimport sys\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.multiprocessing as mp\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom tensorboardX import SummaryWriter\nfrom sklearn.metrics import confusion_matrix\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    \n    # switch to train mode\n    model.train()\n    train_loss = 0\n    for i, (input, target) in enumerate(train_loader):\n        input = input.to(device)\n        target = target.to(device)\n\n        # compute output\n        output = model(input)\n        loss = criterion(output, target)\n        train_loss += loss\n        \n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    train_loss = train_loss/len(train_loader)\n    print(epoch)\n    print(\"train_loss: \", train_loss.cpu().item() )\n    return train_loss\n   \n\ndef validate(val_loader, model, criterion, epoch,flag='val'):\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    # switch to evaluate mode\n    model.eval()\n    all_preds = []\n    all_targets = []\n    val_loss = 0\n    with torch.no_grad():\n       \n        for i, (input, target) in enumerate(val_loader):\n            \n            input = input.to(device)\n            target = target.to(device)\n\n            # compute output\n            output = model(input)\n            loss = criterion(output, target)\n            val_loss += loss\n            \n            _, pred = torch.max(output, 1)\n            all_preds.extend(pred.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n      \n        cf = confusion_matrix(all_targets, all_preds).astype(float)\n        cls_cnt = cf.sum(axis=1)\n        cls_hit = np.diag(cf)\n        cls_acc = cls_hit / cls_cnt\n        \n        acc = sum(cls_hit)/sum(cls_cnt)\n        val_loss = val_loss/len(val_loader)\n        print(\"val_loss: \", (val_loss).cpu().item())\n        out_cls_acc = '%s Class Accuracy: %s'%(flag,(np.array2string(cls_acc, separator=',', formatter={'float_kind':lambda x: \"%.3f\" % x})))\n        \n        print(out_cls_acc)\n        print(acc)\n       \n        \n        # precision_score, recall_score, f1_score, accuracy_score\n        \n        accuracy = accuracy_score(all_targets, all_preds)\n        precision = precision_score(all_targets, all_preds, average=\"macro\")\n        recall = recall_score(all_targets, all_preds, average=\"macro\")\n        f1 = f1_score(all_targets, all_preds, average=\"macro\")\n        error = 1-accuracy\n        \n        return np.array([error, accuracy, precision, recall, f1, val_loss])\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter(object):\n    \n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n\n    def __init__(self, dataset, indices=None, num_samples=None):\n                \n        # if indices is not provided, \n        # all elements in the dataset will be considered\n        self.indices = list(range(len(dataset))) \\\n            if indices is None else indices\n            \n        # if num_samples is not provided, \n        # draw `len(indices)` samples in each iteration\n        self.num_samples = len(self.indices) \\\n            if num_samples is None else num_samples\n            \n        # distribution of classes in the dataset \n        label_to_count = [0] * len(np.unique(dataset.targets))\n        for idx in self.indices:\n            label = self._get_label(dataset, idx)\n            label_to_count[label] += 1\n            \n        beta = 0.9999\n        effective_num = 1.0 - np.power(beta, label_to_count)\n        per_cls_weights = (1.0 - beta) / np.array(effective_num)\n\n        # weight for each sample\n        weights = [per_cls_weights[self._get_label(dataset, idx)]\n                   for idx in self.indices]\n        self.weights = torch.DoubleTensor(weights)\n        \n    def _get_label(self, dataset, idx):\n        return dataset.targets[idx]\n                \n    def __iter__(self):\n        return iter(torch.multinomial(self.weights, self.num_samples, replacement=True).tolist())\n\n    def __len__(self):\n        return self.num_samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############\n# main\n##############\nfrom sklearn.metrics import confusion_matrix\nif __name__ == '__main__':\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    # data setup\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_val = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n\n    train_dataset = IMBALANCECIFAR10(root='../input/cifar10-python', imb_type='exp', imb_factor=0.01,\n                                     rand_number=0, train=True,\n                                     transform=transform_train)\n\n    val_dataset = datasets.CIFAR10(root='../input/cifar10-python', train=False, transform=transform_val)\n\n    print(len(train_dataset))  # 20431\n    print(len(val_dataset))  # 10000\n    cls_num_list = train_dataset.get_cls_num_list()\n    print('cls num list:')\n    print(cls_num_list)\n\n\n\n\n    \n    \n    loss_type = 'CE'\n    train_rule = 'None'\n      \n    train_sampler = None\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=32, shuffle=(train_sampler is None),\n        num_workers=4, pin_memory=True, sampler=train_sampler)\n\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=32, shuffle=False,\n        num_workers=4, pin_memory=True)\n    \n    use_norm = True if loss_type == 'LDAM' else False\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = resnet32(num_classes=len(cls_num_list), use_norm=use_norm).to(device)\n\n    optimizer = optim.Adam(model.parameters())\n    optimizer = torch.optim.SGD(model.parameters(),0.1,\n                               momentum=0.9,\n                             weight_decay=2e-5)\n\n    losses = []\n    epochs =120\n\n\n    batches = len(train_loader)\n    val_batches = len(val_loader)\n\n    best_acc1 = 0\n    best_epoch = 0\n    \n    results = np.array([0,0,0,0,0,0,0])\n    \n    for epoch in range(0, epochs + 1):\n        \n        adjust_learning_rate(optimizer, epoch, 0.1)\n        # define train rule\n        \n        \n\n        if train_rule == 'None':\n            train_sampler = None\n            per_cls_weights = None\n        elif train_rule == 'Resample':\n            train_sampler = ImbalancedDatasetSampler(train_dataset)\n            per_cls_weights = None\n        elif train_rule == 'Reweight':\n            train_sampler = None\n            beta = 0.9999\n            effective_num = 1.0 - np.power(beta, cls_num_list)\n            per_cls_weights = (1.0 - beta) / np.array(effective_num)\n            per_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(cls_num_list)\n            per_cls_weights = torch.FloatTensor(per_cls_weights).to(device)\n        elif train_rule == 'DRW':\n            train_sampler = None\n            idx = epoch // 160\n            betas = [0, 0.9999]\n            effective_num = 1.0 - np.power(betas[idx], cls_num_list)\n            per_cls_weights = (1.0 - betas[idx]) / np.array(effective_num)\n            per_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(cls_num_list)\n            per_cls_weights = torch.FloatTensor(per_cls_weights).to(device)\n        else:\n            warnings.warn('Sample rule is not listed')\n            \n            \n        # define loss function: focal loss, cross entropy\n        \n        criterion = None\n        if loss_type == 'CE':\n            criterion = nn.CrossEntropyLoss(weight=per_cls_weights).to(device)\n        elif loss_type == 'LDAM':\n            criterion = LDAMLoss(cls_num_list=cls_num_list, max_m=0.5, s=30, weight=per_cls_weights).to(device)\n        elif loss_type == 'Focal':\n            criterion = FocalLoss(weight=per_cls_weights, gamma=1).to(device)\n        else:\n            warnings.warn('Loss type is not listed')\n        \n        \n        # train for one epoch\n        train_loss = train(train_loader, model, criterion, optimizer, epoch)\n       \n        \n        \n        # evaluate on validation set\n        current = validate(val_loader, model, criterion, epoch)\n        current = np.append(current, np.array([train_loss]))\n        results = np.vstack((results, current))\n        \n        \n    \n    #write the results error, accuracy, precision, recall, f1\n    import pandas as pd\n    df = pd.DataFrame({\"error\": results[:,0], \n                       \"accuracy\": results[:,1], \n                       \"precision\": results[:,2], \n                       \"recall\": results[:,3], \n                       \"f1\":results[:,4], \n                       \"val_loss\": results[:,5], \"train_loss\":results[:,6]})\n    df.to_csv(r\"name.csv\")\n\n    from IPython.display import FileLink\n    FileLink(r'name.csv')\n    \n        \n        \n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}